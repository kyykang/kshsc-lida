#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LIDA æœ¬åœ°LLMå¤§æ¨¡å‹é…ç½®è„šæœ¬
æ”¯æŒå¤šç§æœ¬åœ°æ¨¡å‹é…ç½®æ–¹å¼ï¼Œæ— éœ€OpenAI APIå¯†é’¥

ä½œè€…: AIåŠ©æ‰‹
æ—¥æœŸ: 2024
"""

import os
import sys
from lida import Manager, llm
from llmx import TextGenerationConfig

def setup_huggingface_model():
    """
    é…ç½®HuggingFaceæœ¬åœ°æ¨¡å‹
    è¿™ç§æ–¹å¼ç›´æ¥ä½¿ç”¨HuggingFaceçš„transformersåº“åŠ è½½æ¨¡å‹
    éœ€è¦GPUæ”¯æŒä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
    """
    print("ğŸ¤– é…ç½®HuggingFaceæœ¬åœ°æ¨¡å‹...")
    print("æ³¨æ„ï¼šè¿™éœ€è¦è¾ƒå¤§çš„GPUå†…å­˜å’Œå­˜å‚¨ç©ºé—´")
    
    # æ¨èçš„å¼€æºæ¨¡å‹åˆ—è¡¨
    recommended_models = [
        "microsoft/DialoGPT-medium",  # è½»é‡çº§å¯¹è¯æ¨¡å‹
        "microsoft/DialoGPT-large",   # å¤§å‹å¯¹è¯æ¨¡å‹
        "uukuguy/speechless-llama2-hermes-orca-platypus-13b",  # é«˜è´¨é‡13Bæ¨¡å‹
        "codellama/CodeLlama-7b-Instruct-hf",  # ä»£ç ç”Ÿæˆä¸“ç”¨æ¨¡å‹
        "WizardLM/WizardCoder-Python-7B-V1.0",  # Pythonä»£ç ä¸“ç”¨æ¨¡å‹
    ]
    
    print("\næ¨èçš„å¼€æºæ¨¡å‹:")
    for i, model in enumerate(recommended_models, 1):
        print(f"{i}. {model}")
    
    print("\nè¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ (è¾“å…¥æ•°å­— 1-5):")
    try:
        choice = int(input("é€‰æ‹©: ")) - 1
        if 0 <= choice < len(recommended_models):
            selected_model = recommended_models[choice]
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            selected_model = recommended_models[0]
    except ValueError:
        print("æ— æ•ˆè¾“å…¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
        selected_model = recommended_models[0]
    
    print(f"\né€‰æ‹©çš„æ¨¡å‹: {selected_model}")
    
    try:
        # åˆ›å»ºHuggingFaceæ–‡æœ¬ç”Ÿæˆå™¨
        print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        text_gen = llm(
            provider="hf", 
            model=selected_model, 
            device_map="auto"  # è‡ªåŠ¨åˆ†é…GPU/CPU
        )
        
        # åˆ›å»ºLIDAç®¡ç†å™¨
        lida_manager = Manager(text_gen=text_gen)
        
        print("âœ… HuggingFaceæ¨¡å‹é…ç½®æˆåŠŸï¼")
        return lida_manager, text_gen
        
    except Exception as e:
        print(f"âŒ HuggingFaceæ¨¡å‹é…ç½®å¤±è´¥: {e}")
        print("å¯èƒ½çš„åŸå› :")
        print("1. ç¼ºå°‘transformersåº“: pip install transformers")
        print("2. ç¼ºå°‘torchåº“: pip install torch")
        print("3. GPUå†…å­˜ä¸è¶³")
        print("4. ç½‘ç»œè¿æ¥é—®é¢˜")
        return None, None

def setup_vllm_server():
    """
    é…ç½®vLLMæœåŠ¡å™¨è¿æ¥
    è¿™ç§æ–¹å¼è¿æ¥åˆ°æœ¬åœ°è¿è¡Œçš„vLLMæœåŠ¡å™¨
    éœ€è¦å…ˆå¯åŠ¨vLLMæœåŠ¡å™¨
    """
    print("ğŸš€ é…ç½®vLLMæœåŠ¡å™¨è¿æ¥...")
    
    # é»˜è®¤é…ç½®
    default_host = "localhost"
    default_port = "8000"
    default_model = "microsoft/DialoGPT-medium"
    
    print(f"\nè¯·è¾“å…¥vLLMæœåŠ¡å™¨é…ç½® (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼):")
    
    host = input(f"æœåŠ¡å™¨åœ°å€ [{default_host}]: ").strip() or default_host
    port = input(f"ç«¯å£å· [{default_port}]: ").strip() or default_port
    model_name = input(f"æ¨¡å‹åç§° [{default_model}]: ").strip() or default_model
    
    api_base = f"http://{host}:{port}/v1"
    
    print(f"\né…ç½®ä¿¡æ¯:")
    print(f"APIåœ°å€: {api_base}")
    print(f"æ¨¡å‹åç§°: {model_name}")
    
    try:
        # æ¨¡å‹è¯¦ç»†ä¿¡æ¯é…ç½®
        model_details = [{
            'name': model_name, 
            'max_tokens': 2596, 
            'model': {
                'provider': 'openai', 
                'parameters': {'model': model_name}
            }
        }]
        
        # åˆ›å»ºæ–‡æœ¬ç”Ÿæˆå™¨
        text_gen = llm(
            provider="openai",  
            api_base=api_base, 
            api_key="EMPTY",  # vLLMä¸éœ€è¦çœŸå®APIå¯†é’¥
            models=model_details
        )
        
        # åˆ›å»ºLIDAç®¡ç†å™¨
        lida_manager = Manager(text_gen=text_gen)
        
        print("âœ… vLLMæœåŠ¡å™¨é…ç½®æˆåŠŸï¼")
        print("\nå¯åŠ¨vLLMæœåŠ¡å™¨çš„å‘½ä»¤ç¤ºä¾‹:")
        print(f"python -m vllm.entrypoints.openai.api_server --model {model_name} --port {port}")
        
        return lida_manager, text_gen
        
    except Exception as e:
        print(f"âŒ vLLMæœåŠ¡å™¨é…ç½®å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿:")
        print("1. vLLMæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
        print("2. æœåŠ¡å™¨åœ°å€å’Œç«¯å£æ­£ç¡®")
        print("3. æ¨¡å‹åç§°æ­£ç¡®")
        return None, None

def setup_ollama_server():
    """
    é…ç½®OllamaæœåŠ¡å™¨è¿æ¥
    Ollamaæ˜¯ä¸€ä¸ªè½»é‡çº§çš„æœ¬åœ°LLMè¿è¡Œå·¥å…·
    """
    print("ğŸ¦™ é…ç½®OllamaæœåŠ¡å™¨è¿æ¥...")
    
    default_host = "localhost"
    default_port = "11434"
    default_model = "llama2"
    
    print(f"\nè¯·è¾“å…¥OllamaæœåŠ¡å™¨é…ç½®:")
    
    host = input(f"æœåŠ¡å™¨åœ°å€ [{default_host}]: ").strip() or default_host
    port = input(f"ç«¯å£å· [{default_port}]: ").strip() or default_port
    model_name = input(f"æ¨¡å‹åç§° [{default_model}]: ").strip() or default_model
    
    api_base = f"http://{host}:{port}/v1"
    
    print(f"\né…ç½®ä¿¡æ¯:")
    print(f"APIåœ°å€: {api_base}")
    print(f"æ¨¡å‹åç§°: {model_name}")
    
    try:
        # æ¨¡å‹é…ç½®
        model_details = [{
            'name': model_name,
            'max_tokens': 2048,
            'model': {
                'provider': 'openai',
                'parameters': {'model': model_name}
            }
        }]
        
        # åˆ›å»ºæ–‡æœ¬ç”Ÿæˆå™¨
        text_gen = llm(
            provider="openai",
            api_base=api_base,
            api_key="ollama",  # Ollamaä½¿ç”¨å›ºå®šå¯†é’¥
            models=model_details
        )
        
        # åˆ›å»ºLIDAç®¡ç†å™¨
        lida_manager = Manager(text_gen=text_gen)
        
        print("âœ… OllamaæœåŠ¡å™¨é…ç½®æˆåŠŸï¼")
        print("\nå®‰è£…å’Œå¯åŠ¨Ollamaçš„å‘½ä»¤:")
        print("1. å®‰è£…: curl -fsSL https://ollama.ai/install.sh | sh")
        print(f"2. ä¸‹è½½æ¨¡å‹: ollama pull {model_name}")
        print("3. å¯åŠ¨æœåŠ¡: ollama serve")
        
        return lida_manager, text_gen
        
    except Exception as e:
        print(f"âŒ OllamaæœåŠ¡å™¨é…ç½®å¤±è´¥: {e}")
        return None, None

def test_local_model(lida_manager):
    """
    æµ‹è¯•æœ¬åœ°æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
    """
    print("\nğŸ§ª æµ‹è¯•æœ¬åœ°æ¨¡å‹...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        import pandas as pd
        test_data = pd.DataFrame({
            'name': ['Alice', 'Bob', 'Charlie'],
            'age': [25, 30, 35],
            'salary': [50000, 60000, 70000]
        })
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        test_file = "/tmp/test_data.csv"
        test_data.to_csv(test_file, index=False)
        
        print("æ­£åœ¨ç”Ÿæˆæ•°æ®æ‘˜è¦...")
        
        # æµ‹è¯•æ•°æ®æ‘˜è¦åŠŸèƒ½
        summary = lida_manager.summarize(
            data=test_file,
            file_name="test_data.csv",
            summary_method="default"  # ä½¿ç”¨åŸºç¡€æ‘˜è¦ï¼Œä¸ä¾èµ–LLM
        )
        
        print("âœ… æœ¬åœ°æ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
        print(f"æ•°æ®æ‘˜è¦: {summary.name}")
        print(f"æ•°æ®è¡Œæ•°: {summary.shape[0]}")
        print(f"æ•°æ®åˆ—æ•°: {summary.shape[1]}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æœ¬åœ°æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def save_config(config_type, config_data):
    """
    ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    """
    config_file = "local_llm_config.py"
    
    config_content = f'''
# LIDA æœ¬åœ°LLMé…ç½®æ–‡ä»¶
# é…ç½®ç±»å‹: {config_type}
# ç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now()}

from lida import Manager, llm

def get_lida_manager():
    """è·å–é…ç½®å¥½çš„LIDAç®¡ç†å™¨"""
'''
    
    if config_type == "huggingface":
        config_content += f'''
    text_gen = llm(
        provider="hf",
        model="{config_data['model']}",
        device_map="auto"
    )
'''
    elif config_type == "vllm":
        config_content += f'''
    model_details = [{{
        'name': "{config_data['model']}",
        'max_tokens': 2596,
        'model': {{
            'provider': 'openai',
            'parameters': {{'model': "{config_data['model']}"}}
        }}
    }}]
    
    text_gen = llm(
        provider="openai",
        api_base="{config_data['api_base']}",
        api_key="EMPTY",
        models=model_details
    )
'''
    elif config_type == "ollama":
        config_content += f'''
    model_details = [{{
        'name': "{config_data['model']}",
        'max_tokens': 2048,
        'model': {{
            'provider': 'openai',
            'parameters': {{'model': "{config_data['model']}"}}
        }}
    }}]
    
    text_gen = llm(
        provider="openai",
        api_base="{config_data['api_base']}",
        api_key="ollama",
        models=model_details
    )
'''
    
    config_content += '''
    return Manager(text_gen=text_gen)

if __name__ == "__main__":
    lida = get_lida_manager()
    print("LIDAæœ¬åœ°æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼")
'''
    
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    print(f"\nğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {config_file}")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("from local_llm_config import get_lida_manager")
    print("lida = get_lida_manager()")

def main():
    """
    ä¸»å‡½æ•°ï¼šæ˜¾ç¤ºèœå•å¹¶å¤„ç†ç”¨æˆ·é€‰æ‹©
    """
    print("ğŸš€ LIDA æœ¬åœ°LLMå¤§æ¨¡å‹é…ç½®å·¥å…·")
    print("=" * 50)
    print("é€‰æ‹©é…ç½®æ–¹å¼:")
    print("1. HuggingFaceæ¨¡å‹ (ç›´æ¥åŠ è½½)")
    print("2. vLLMæœåŠ¡å™¨ (OpenAIå…¼å®¹API)")
    print("3. OllamaæœåŠ¡å™¨ (è½»é‡çº§æœ¬åœ°LLM)")
    print("4. é€€å‡º")
    
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹© (1-4): ").strip()
            
            if choice == "1":
                lida_manager, text_gen = setup_huggingface_model()
                if lida_manager:
                    if test_local_model(lida_manager):
                        save_config("huggingface", {"model": text_gen.model})
                    break
                    
            elif choice == "2":
                lida_manager, text_gen = setup_vllm_server()
                if lida_manager:
                    save_config("vllm", {
                        "model": text_gen.models[0]['name'],
                        "api_base": text_gen.api_base
                    })
                    break
                    
            elif choice == "3":
                lida_manager, text_gen = setup_ollama_server()
                if lida_manager:
                    save_config("ollama", {
                        "model": text_gen.models[0]['name'],
                        "api_base": text_gen.api_base
                    })
                    break
                    
            elif choice == "4":
                print("ğŸ‘‹ é€€å‡ºé…ç½®å·¥å…·")
                sys.exit(0)
                
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-4")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            sys.exit(0)
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    
    print("\nğŸ‰ æœ¬åœ°LLMé…ç½®å®Œæˆï¼")
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("1. ä½¿ç”¨ç”Ÿæˆçš„é…ç½®æ–‡ä»¶å¯åŠ¨LIDA")
    print("2. æˆ–è€…ç›´æ¥è¿è¡Œ: python local_llm_config.py")
    print("3. å¯åŠ¨Webç•Œé¢: lida ui --port=8080")

if __name__ == "__main__":
    main()
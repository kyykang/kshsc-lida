# LIDA æœ¬åœ°LLMå¤§æ¨¡å‹ä½¿ç”¨æŒ‡å—

## ğŸ“– æ¦‚è¿°

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ é…ç½®å’Œä½¿ç”¨æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¿è¡ŒLIDAï¼Œæ— éœ€OpenAI APIå¯†é’¥ï¼Œå®Œå…¨å…è´¹ä½¿ç”¨ï¼

## ğŸ¯ ä¼˜åŠ¿

âœ… **å®Œå…¨å…è´¹** - æ— éœ€ä»˜è´¹APIå¯†é’¥  
âœ… **æ•°æ®éšç§** - æ•°æ®ä¸ä¼šå‘é€åˆ°å¤–éƒ¨æœåŠ¡å™¨  
âœ… **ç¦»çº¿ä½¿ç”¨** - é…ç½®å®Œæˆåå¯ç¦»çº¿è¿è¡Œ  
âœ… **è‡ªå®šä¹‰æ§åˆ¶** - å¯ä»¥é€‰æ‹©é€‚åˆçš„æ¨¡å‹å¤§å°å’Œæ€§èƒ½  

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨é…ç½®å·¥å…·ï¼ˆæ¨èæ–°æ‰‹ï¼‰

```bash
# è¿›å…¥LIDAç›®å½•
cd lida-main

# è¿è¡Œé…ç½®å·¥å…·
python setup_local_llm.py
```

æŒ‰ç…§æç¤ºé€‰æ‹©é…ç½®æ–¹å¼ï¼Œå·¥å…·ä¼šè‡ªåŠ¨å¸®ä½ å®Œæˆé…ç½®ã€‚

### æ–¹æ³•äºŒï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èæœ‰ç»éªŒç”¨æˆ·ï¼‰

```bash
# ä½¿ç”¨HuggingFaceæ¨¡å‹ï¼ˆæ¨èï¼‰
python start_with_local_llm.py --model huggingface

# ä½¿ç”¨Ollamaæ¨¡å‹
python start_with_local_llm.py --model ollama

# ä½¿ç”¨vLLMæœåŠ¡å™¨
python start_with_local_llm.py --model vllm
```

## ğŸ¤– æ”¯æŒçš„æœ¬åœ°æ¨¡å‹ç±»å‹

### 1. HuggingFace æ¨¡å‹ï¼ˆæ¨èæ–°æ‰‹ï¼‰

**ä¼˜ç‚¹ï¼š**
- é…ç½®ç®€å•ï¼Œä¸€é”®å¯åŠ¨
- æ¨¡å‹è´¨é‡é«˜
- ç¤¾åŒºæ”¯æŒå¥½

**ç¼ºç‚¹ï¼š**
- é¦–æ¬¡ä¸‹è½½æ¨¡å‹éœ€è¦æ—¶é—´
- éœ€è¦è¾ƒå¤šå†…å­˜

**æ¨èæ¨¡å‹ï¼š**
- `microsoft/DialoGPT-medium` - è½»é‡çº§ï¼Œé€‚åˆå…¥é—¨
- `microsoft/DialoGPT-large` - æ•ˆæœæ›´å¥½ï¼Œéœ€è¦æ›´å¤šå†…å­˜
- `codellama/CodeLlama-7b-Instruct-hf` - ä¸“é—¨ç”¨äºä»£ç ç”Ÿæˆ

**ä½¿ç”¨æ–¹æ³•ï¼š**
```python
from lida import Manager, llm

# åˆ›å»ºHuggingFaceæ–‡æœ¬ç”Ÿæˆå™¨
text_gen = llm(
    provider="hf",
    model="microsoft/DialoGPT-medium",
    device_map="auto"  # è‡ªåŠ¨åˆ†é…GPU/CPU
)

# åˆ›å»ºLIDAç®¡ç†å™¨
lida = Manager(text_gen=text_gen)
```

### 2. Ollama æœåŠ¡å™¨ï¼ˆæ¨èæ—¥å¸¸ä½¿ç”¨ï¼‰

**ä¼˜ç‚¹ï¼š**
- è½»é‡çº§ï¼Œèµ„æºå ç”¨å°‘
- å¯åŠ¨å¿«é€Ÿ
- æ”¯æŒå¤šç§å¼€æºæ¨¡å‹

**ç¼ºç‚¹ï¼š**
- éœ€è¦å•ç‹¬å®‰è£…Ollama
- éœ€è¦æ‰‹åŠ¨ç®¡ç†æ¨¡å‹

**å®‰è£…æ­¥éª¤ï¼š**
```bash
# 1. å®‰è£…Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. å¯åŠ¨OllamaæœåŠ¡
ollama serve

# 3. ä¸‹è½½æ¨¡å‹ï¼ˆæ–°å¼€ç»ˆç«¯çª—å£ï¼‰
ollama pull llama2        # åŸºç¡€æ¨¡å‹
ollama pull codellama     # ä»£ç ä¸“ç”¨æ¨¡å‹
ollama pull mistral       # é«˜æ€§èƒ½æ¨¡å‹
```

**ä½¿ç”¨æ–¹æ³•ï¼š**
```python
from lida import Manager, llm

# é…ç½®Ollamaè¿æ¥
model_details = [{
    'name': "llama2",
    'max_tokens': 2048,
    'model': {
        'provider': 'openai',
        'parameters': {'model': "llama2"}
    }
}]

text_gen = llm(
    provider="openai",
    api_base="http://localhost:11434/v1",
    api_key="ollama",
    models=model_details
)

lida = Manager(text_gen=text_gen)
```

### 3. vLLM æœåŠ¡å™¨ï¼ˆæ¨èé«˜æ€§èƒ½éœ€æ±‚ï¼‰

**ä¼˜ç‚¹ï¼š**
- é«˜æ€§èƒ½æ¨ç†
- æ”¯æŒæ‰¹é‡å¤„ç†
- OpenAIå…¼å®¹API

**ç¼ºç‚¹ï¼š**
- é…ç½®ç›¸å¯¹å¤æ‚
- éœ€è¦è¾ƒå¥½çš„ç¡¬ä»¶

**å®‰è£…æ­¥éª¤ï¼š**
```bash
# 1. å®‰è£…vLLM
pip install vllm

# 2. å¯åŠ¨vLLMæœåŠ¡å™¨
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/DialoGPT-medium \
    --port 8000
```

**ä½¿ç”¨æ–¹æ³•ï¼š**
```python
from lida import Manager, llm

# é…ç½®vLLMè¿æ¥
model_details = [{
    'name': "microsoft/DialoGPT-medium",
    'max_tokens': 2596,
    'model': {
        'provider': 'openai',
        'parameters': {'model': "microsoft/DialoGPT-medium"}
    }
}]

text_gen = llm(
    provider="openai",
    api_base="http://localhost:8000/v1",
    api_key="EMPTY",
    models=model_details
)

lida = Manager(text_gen=text_gen)
```

## ğŸ’» ç³»ç»Ÿè¦æ±‚

### æœ€ä½é…ç½®
- **å†…å­˜ï¼š** 8GB RAM
- **å­˜å‚¨ï¼š** 10GB å¯ç”¨ç©ºé—´
- **Pythonï¼š** 3.9+

### æ¨èé…ç½®
- **å†…å­˜ï¼š** 16GB+ RAM
- **æ˜¾å¡ï¼š** NVIDIA GPUï¼ˆå¯é€‰ï¼ŒåŠ é€Ÿæ¨ç†ï¼‰
- **å­˜å‚¨ï¼š** 20GB+ å¯ç”¨ç©ºé—´
- **ç½‘ç»œï¼š** é¦–æ¬¡ä¸‹è½½æ¨¡å‹éœ€è¦ç¨³å®šç½‘ç»œ

## ğŸ› ï¸ å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install -U lida llmx

# HuggingFaceæ”¯æŒ
pip install transformers torch

# GPUæ”¯æŒï¼ˆå¯é€‰ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# vLLMæ”¯æŒï¼ˆå¯é€‰ï¼‰
pip install vllm
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„æ•°æ®å¯è§†åŒ–æµç¨‹

```python
from lida import Manager, llm
import pandas as pd

# 1. é…ç½®æœ¬åœ°æ¨¡å‹
text_gen = llm(
    provider="hf",
    model="microsoft/DialoGPT-medium",
    device_map="auto"
)

lida = Manager(text_gen=text_gen)

# 2. å‡†å¤‡æ•°æ®
data = pd.DataFrame({
    'month': ['Jan', 'Feb', 'Mar', 'Apr', 'May'],
    'sales': [100, 120, 140, 110, 160],
    'profit': [20, 25, 30, 22, 35]
})
data.to_csv('sales_data.csv', index=False)

# 3. ç”Ÿæˆæ•°æ®æ‘˜è¦
summary = lida.summarize(
    data='sales_data.csv',
    file_name='sales_data.csv',
    summary_method='default'  # ä½¿ç”¨åŸºç¡€æ‘˜è¦ï¼Œä¸ä¾èµ–LLM
)

print(f"æ•°æ®æ‘˜è¦: {summary.name}")
print(f"è¡Œæ•°: {summary.shape[0]}, åˆ—æ•°: {summary.shape[1]}")

# 4. ç”Ÿæˆå¯è§†åŒ–ç›®æ ‡ï¼ˆéœ€è¦LLMï¼‰
try:
    goals = lida.goals(summary, n=3)
    print(f"ç”Ÿæˆäº† {len(goals)} ä¸ªå¯è§†åŒ–ç›®æ ‡")
    
    # 5. ç”Ÿæˆå›¾è¡¨
    if goals:
        charts = lida.visualize(
            summary=summary,
            goal=goals[0],
            library='matplotlib'
        )
        print(f"ç”Ÿæˆäº† {len(charts)} ä¸ªå›¾è¡¨")
except Exception as e:
    print(f"LLMåŠŸèƒ½å‡ºé”™: {e}")
    print("å¯ä»¥ä½¿ç”¨åŸºç¡€æ‘˜è¦åŠŸèƒ½ï¼Œä½†éœ€è¦æ‰‹åŠ¨åˆ›å»ºå¯è§†åŒ–ç›®æ ‡")
```

### å¯åŠ¨Webç•Œé¢

```bash
# ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¯åŠ¨Webç•Œé¢
python start_with_local_llm.py --model huggingface --port 8080

# ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—®
# http://localhost:8080
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: æ¨¡å‹ä¸‹è½½å¾ˆæ…¢æˆ–å¤±è´¥ï¼Ÿ**
A: 
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ä½¿ç”¨å›½å†…é•œåƒï¼š`export HF_ENDPOINT=https://hf-mirror.com`
- å°è¯•æ›´å°çš„æ¨¡å‹

**Q: å†…å­˜ä¸è¶³é”™è¯¯ï¼Ÿ**
A:
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚DialoGPT-mediumè€Œä¸æ˜¯largeï¼‰
- å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„ç¨‹åº
- ä½¿ç”¨CPUè€Œä¸æ˜¯GPUï¼š`device_map="cpu"`

**Q: Ollamaè¿æ¥å¤±è´¥ï¼Ÿ**
A:
- ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œï¼š`ollama serve`
- æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
- ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½ï¼š`ollama list`

**Q: ç”Ÿæˆçš„å›¾è¡¨è´¨é‡ä¸å¥½ï¼Ÿ**
A:
- å°è¯•ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
- è°ƒæ•´æ¸©åº¦å‚æ•°ï¼š`TextGenerationConfig(temperature=0.7)`
- ä½¿ç”¨ä¸“é—¨çš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚CodeLlamaï¼‰

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨GPUåŠ é€Ÿ**
   ```python
   # ç¡®ä¿å®‰è£…äº†CUDAç‰ˆæœ¬çš„PyTorch
   text_gen = llm(
       provider="hf",
       model="microsoft/DialoGPT-medium",
       device_map="cuda"  # å¼ºåˆ¶ä½¿ç”¨GPU
   )
   ```

2. **è°ƒæ•´æ¨¡å‹å‚æ•°**
   ```python
   from lida.datamodel import TextGenerationConfig
   
   config = TextGenerationConfig(
       temperature=0.3,    # é™ä½éšæœºæ€§
       max_tokens=1000,    # é™åˆ¶è¾“å‡ºé•¿åº¦
       n=1                 # åªç”Ÿæˆä¸€ä¸ªç»“æœ
   )
   ```

3. **ä½¿ç”¨ç¼“å­˜**
   ```python
   config = TextGenerationConfig(use_cache=True)
   goals = lida.goals(summary, textgen_config=config)
   ```

## ğŸ‰ æ€»ç»“

é€šè¿‡æœ¬æŒ‡å—ï¼Œä½ ç°åœ¨å¯ä»¥ï¼š

1. âœ… é€‰æ‹©é€‚åˆçš„æœ¬åœ°LLMæ¨¡å‹
2. âœ… é…ç½®å’Œå¯åŠ¨æœ¬åœ°æ¨¡å‹æœåŠ¡
3. âœ… ä½¿ç”¨LIDAè¿›è¡Œæ•°æ®å¯è§†åŒ–
4. âœ… è§£å†³å¸¸è§é—®é¢˜å’Œä¼˜åŒ–æ€§èƒ½

**æ¨èå­¦ä¹ è·¯å¾„ï¼š**
1. æ–°æ‰‹ï¼šä»HuggingFaceæ¨¡å‹å¼€å§‹
2. è¿›é˜¶ï¼šå°è¯•Ollamaè·å¾—æ›´å¥½çš„æ€§èƒ½
3. é«˜çº§ï¼šä½¿ç”¨vLLMè¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

**ä¸‹ä¸€æ­¥ï¼š**
- å°è¯•ä¸åŒçš„æ¨¡å‹å’Œå‚æ•°
- æ¢ç´¢LIDAçš„é«˜çº§åŠŸèƒ½
- é›†æˆåˆ°ä½ çš„æ•°æ®åˆ†æå·¥ä½œæµä¸­

ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ğŸš€